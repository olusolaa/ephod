{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11520968,"sourceType":"datasetVersion","datasetId":7225559}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.031449Z","iopub.execute_input":"2025-04-24T02:02:50.032110Z","iopub.status.idle":"2025-04-24T02:02:50.041828Z","shell.execute_reply.started":"2025-04-24T02:02:50.032082Z","shell.execute_reply":"2025-04-24T02:02:50.041141Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ephod-dataset/pHopt_data.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# # SequenceHacking: LSTM-RLAT for Enzyme pH Prediction\n#\n# This notebook trains a model combining an LSTM feature extractor with the Residual Light Attention (RLAT) architecture (adapted from EpHod) to predict enzyme optimal pH from amino acid sequences.\n#\n# **Steps:**\n# 1.  **Setup:** Import libraries.\n# 2.  **Configuration:** Set hyperparameters and file paths. **Upload your dataset to Kaggle first!**\n# 3.  **Module Definitions:** Define LSTM, RLAT utilities, Combined Model, Dataset, Loss, and Training functions.\n# 4.  **Training:** Run the main training loop.\n# 5.  **Evaluation (Optional):** Load the best model and evaluate on the test set.","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 1. Setup Cell: Imports\n# =============================================================================\nimport numpy as np\nimport pandas as pd\nimport time\nimport json\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm.notebook import tqdm # Use notebook tqdm\n\n# Imports potentially needed by trainutils (adjust if needed)\nfrom scipy.stats import spearmanr, pearsonr\nfrom scipy.ndimage import convolve1d, gaussian_filter1d\nfrom sklearn import metrics\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.043071Z","iopub.execute_input":"2025-04-24T02:02:50.043263Z","iopub.status.idle":"2025-04-24T02:02:50.050859Z","shell.execute_reply.started":"2025-04-24T02:02:50.043248Z","shell.execute_reply":"2025-04-24T02:02:50.050312Z"}},"outputs":[{"name":"stdout","text":"PyTorch Version: 2.5.1+cu124\nCUDA Available: True\nCUDA Device Name: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# =============================================================================\n# 2. Configuration Cell: Set Parameters\n# =============================================================================\n\n# --- Data Paths ---\n# IMPORTANT: Upload your dataset (e.g., pHopt_data.csv) to Kaggle first.\n# Then, find its path under /kaggle/input/your-dataset-name/\n# Example: If you uploaded it as \"ephod-data\", the path might be:\n# TARGET_DATA_PATH = '/kaggle/input/ephod-data/pHopt_data.csv'\nTARGET_DATA_PATH = '/kaggle/input/ephod-dataset/pHopt_data.csv' # <<< CHANGE THIS\n\n# --- Model Hyperparameters ---\nLSTM_EMB_DIM = 128       # Embedding dimension for LSTM input\nLSTM_HIDDEN_DIM = 256    # Hidden dimension per direction for BiLSTM (Output will be 2 * this)\nLSTM_LAYERS = 2          # Number of BiLSTM layers\nLSTM_DROPOUT = 0.2       # Dropout rate for LSTM\nRLAT_KERNEL_SIZE = 7     # Kernel size for RLAT convolutions\nRLAT_DROPOUT = 0.3       # Dropout rate for RLAT dense layers (tune this)\nRLAT_RES_BLOCKS = 4      # Number of residual blocks in RLAT\nRLAT_ACTIVATION = 'elu'  # Activation function for RLAT\n\n# --- Training Parameters ---\nLEARNING_RATE = 1e-4       # Initial learning rate (tune this)\nL2_REG = 1e-5            # L2 regularization (weight decay)\nBATCH_SIZE = 32            # Batch size for training (adjust based on GPU memory)\nEPOCHS = 200               # Maximum number of training epochs\nSAMPLE_WEIGHT_METHOD = 'LDS_inv_sqrt' # Method for sample weighting (e.g., None, bin_inv, LDS_inv_sqrt)\nREDUCE_LR_PATIENCE = 10      # Patience for ReduceLROnPlateau scheduler\nSTOP_PATIENCE = 30         # Patience for early stopping\n\n# --- Infrastructure ---\nNUM_WORKERS = 2            # Number of workers for DataLoader (use 2 or 4 on Kaggle)\nSAVEDIR = '/kaggle/working/' # Directory to save models and logs (Kaggle writable directory)\nMODEL_NAME = 'LSTM_RLAT_v1' # Name for saved model files\nRANDOM_SEED = 42           # Random seed for reproducibility\n\n# Derived config\nLSTM_OUTPUT_DIM = LSTM_HIDDEN_DIM * 2\n\n# --- Create save directory ---\nif not os.path.exists(SAVEDIR):\n    os.makedirs(SAVEDIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.051491Z","iopub.execute_input":"2025-04-24T02:02:50.051715Z","iopub.status.idle":"2025-04-24T02:02:50.065806Z","shell.execute_reply.started":"2025-04-24T02:02:50.051701Z","shell.execute_reply":"2025-04-24T02:02:50.065124Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# =============================================================================\n# 3. LSTM Module Definition Cell (from lstm_module.py)\n# =============================================================================\n\n# Define Vocabulary (Example - ensure it matches protein space + special tokens)\nDEFAULT_AAS = 'ACDEFGHIKLMNPQRSTVWY'\nPAD_TOKEN = '<pad>'\nUNK_TOKEN = '<unk>'\nVOCAB = [PAD_TOKEN] + list(DEFAULT_AAS) + [UNK_TOKEN]\nAA_TO_ID = {aa: i for i, aa in enumerate(VOCAB)}\nVOCAB_SIZE = len(VOCAB)\nPAD_ID = AA_TO_ID[PAD_TOKEN]\n\nclass LSTMFeatureExtractor(nn.Module):\n    \"\"\"LSTM model to generate per-residue features.\"\"\"\n    def __init__(self,\n                 vocab_size=VOCAB_SIZE,\n                 embedding_dim=LSTM_EMB_DIM, # Use config\n                 lstm_hidden_dim=LSTM_HIDDEN_DIM, # Use config\n                 num_lstm_layers=LSTM_LAYERS, # Use config\n                 dropout=LSTM_DROPOUT, # Use config\n                 random_seed=RANDOM_SEED): # Use config\n        super().__init__()\n        _ = torch.manual_seed(random_seed)\n\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.lstm_hidden_dim = lstm_hidden_dim\n        self.num_lstm_layers = num_lstm_layers\n        self.dropout = dropout\n        self.output_dim = lstm_hidden_dim * 2 # BiLSTM concatenates\n\n        self.embedding = nn.Embedding(\n            num_embeddings=self.vocab_size,\n            embedding_dim=self.embedding_dim,\n            padding_idx=PAD_ID\n        )\n        self.embed_dropout = nn.Dropout(dropout)\n        self.lstm = nn.LSTM(\n            input_size=self.embedding_dim,\n            hidden_size=self.lstm_hidden_dim,\n            num_layers=self.num_lstm_layers,\n            bidirectional=True,\n            batch_first=True,\n            dropout=self.dropout if self.num_lstm_layers > 1 else 0\n        )\n\n    def forward(self, input_ids):\n        # input_ids: [batch_size, seq_len]\n        embedded = self.embedding(input_ids)\n        # embedded: [batch_size, seq_len, embedding_dim]\n        embedded = self.embed_dropout(embedded)\n        lstm_out, _ = self.lstm(embedded)\n        # lstm_out: [batch_size, seq_len, lstm_hidden_dim * 2]\n        features_out = lstm_out.transpose(1, 2)\n        # features_out: [batch_size, output_dim, seq_len]\n        return features_out\n\n# --- Helper functions ---\ndef tokenize_sequence(sequence, aa_to_id_map=AA_TO_ID):\n    \"\"\"Converts an amino acid sequence string to a list of token IDs.\"\"\"\n    processed_seq = sequence.upper() # Example processing\n    # Handle common non-standard chars by replacing with UNK\n    for char in ['B', 'J', 'O', 'U', 'Z', 'X', '*']:\n         processed_seq = processed_seq.replace(char, UNK_TOKEN)\n    return [aa_to_id_map.get(aa, AA_TO_ID[UNK_TOKEN]) for aa in processed_seq]\n\ndef pad_sequences(sequences_tokenized, max_len=1024, pad_value=PAD_ID):\n    \"\"\"Pads a list of tokenized sequences to the same length.\"\"\"\n    padded_sequences = []\n    masks = []\n    actual_lengths = []\n    for seq in sequences_tokenized:\n        seq_len = len(seq)\n        truncated_seq = seq[:max_len] # Truncate if longer\n        seq_len = len(truncated_seq) # Update length after potential truncation\n        padding_len = max_len - seq_len\n        padded_seq = truncated_seq + [pad_value] * padding_len\n        mask = [1] * seq_len + [0] * padding_len\n\n        padded_sequences.append(padded_seq)\n        masks.append(mask)\n        actual_lengths.append(seq_len)\n\n    return torch.tensor(padded_sequences, dtype=torch.long), \\\n           torch.tensor(masks, dtype=torch.int32), \\\n           torch.tensor(actual_lengths, dtype=torch.long)\n\nprint(\"LSTM Module and Helpers Defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.066388Z","iopub.execute_input":"2025-04-24T02:02:50.066582Z","iopub.status.idle":"2025-04-24T02:02:50.086001Z","shell.execute_reply.started":"2025-04-24T02:02:50.066567Z","shell.execute_reply":"2025-04-24T02:02:50.085350Z"}},"outputs":[{"name":"stdout","text":"LSTM Module and Helpers Defined.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# %% [code]\n# =============================================================================\n# 4. Train Utils & RLAT Model Definition Cell\n# =============================================================================\n# --- Contains functions from trainutils.py and necessary RLAT definitions ---\n# --- from nn_models.py provided by the user ---\n\n# Imports potentially needed (already in Cell 1, but good practice)\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.stats import spearmanr, pearsonr\nfrom scipy.ndimage import convolve1d, gaussian_filter1d\nfrom sklearn import metrics\n\n# --- RLAT Components (Copied from nn_models.py) ---\n\ndef torchActivation(activation='elu'):\n    '''Return an activation function from torch.nn'''\n    if activation == 'relu':\n        return nn.ReLU()\n    elif activation == 'leaky_relu':\n        return nn.LeakyReLU()\n    elif activation == 'elu':\n        return nn.ELU()\n    elif activation == 'selu':\n        return nn.SELU()\n    elif activation == 'gelu':\n        return nn.GELU()\n    else:\n        raise ValueError(f\"Unsupported activation: {activation}\")\n\nclass ResidualDense(nn.Module):\n    '''A single dense layer with residual connection'''\n    def __init__(self, dim=2560, dropout=0.1, activation='elu', random_seed=0):\n        super(ResidualDense, self).__init__()\n        _ = torch.manual_seed(random_seed)\n        self.dense = nn.Linear(dim, dim)\n        self.batchnorm = nn.BatchNorm1d(dim)\n        self.activation = torchActivation(activation)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x0 = x\n        x = self.dense(x)\n        x = self.batchnorm(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = x0 + x\n        return x\n\nclass LightAttention(nn.Module):\n    '''Convolution model with attention to learn pooled representations from embeddings'''\n    def __init__(self, dim=1280, kernel_size=7, random_seed=0):\n        super(LightAttention, self).__init__()\n        _ = torch.manual_seed(random_seed)\n        # Ensure kernel_size is odd for 'same' padding calculation\n        if kernel_size % 2 == 0:\n             print(f\"Warning: Even kernel_size ({kernel_size}) used in LightAttention. Using floor for padding.\")\n        samepad = kernel_size // 2\n        self.values_conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=samepad)\n        self.weights_conv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=samepad)\n        self.softmax = nn.Softmax(dim=-1) # Apply softmax along the sequence length dimension\n\n    def forward(self, x, mask=None):\n        # x shape: [batch_size, dim, seq_len]\n        # mask shape: [batch_size, seq_len], 0 for pad, 1 for real\n        if mask is None:\n            mask = torch.ones(x.shape[0], x.shape[2], dtype=torch.int32, device=x.device)\n\n        # Ensure mask is on the same device as input x\n        mask = mask.to(x.device)\n\n        # Calculate values and weights\n        values = self.values_conv(x)\n        weights = self.weights_conv(x)\n\n        # Apply mask: Set masked positions to a very small number (-1e6 or -inf)\n        # before softmax for weights and before max pooling for values.\n        # Mask needs to be broadcastable: [batch_size, 1, seq_len]\n        mask_expanded = mask.unsqueeze(1) # Shape: [batch_size, 1, seq_len]\n\n        # Mask values before max pooling\n        values_masked_for_max = values.masked_fill(mask_expanded == 0, -float('inf')) # Use -inf for max\n\n        # Mask weights before softmax\n        weights_masked_for_softmax = weights.masked_fill(mask_expanded == 0, -float('inf')) # Use -inf for softmax\n\n        # Calculate softmax attention weights\n        attention_weights = self.softmax(weights_masked_for_softmax) # Shape: [batch_size, dim, seq_len]\n\n        # Mask values again before weighted sum (optional but safe: prevents NaN if value was -inf)\n        # Using the original values tensor here. Masking ensures padded positions don't contribute.\n        values_masked_for_sum = values.masked_fill(mask_expanded == 0, 0.0) # Use 0 for sum\n\n        # Calculate attention-weighted sum pooling\n        # Element-wise multiply values and attention weights, then sum over sequence length\n        # Sum over the last dimension (seq_len)\n        x_sum = torch.sum(values_masked_for_sum * attention_weights, dim=-1) # Shape: [batch_size, dim]\n\n        # Calculate max pooling over sequence length\n        # Using values masked appropriately for max pooling\n        x_max, _ = torch.max(values_masked_for_max, dim=-1) # Shape: [batch_size, dim]\n        # Handle cases where all values were masked (result is -inf), replace with 0\n        x_max = torch.where(torch.isinf(x_max), torch.zeros_like(x_max), x_max)\n\n\n        # Concatenate sum-pooled and max-pooled representations\n        x_pooled = torch.cat([x_sum, x_max], dim=1) # Shape: [batch_size, 2 * dim]\n\n        return x_pooled, attention_weights # Return pooled representation and weights\n\nclass ResidualLightAttention(nn.Module):\n    '''Model consisting of light attention followed by residual dense layers'''\n    def __init__(self, dim=1280, kernel_size=9, dropout=0.5,\n                 activation='relu', res_blocks=4, random_seed=0):\n        super(ResidualLightAttention, self).__init__()\n        torch.manual_seed(random_seed)\n        self.light_attention = LightAttention(dim, kernel_size, random_seed)\n        # The input to BatchNorm and Dense layers is 2*dim due to concatenation in LightAttention\n        self.batchnorm = nn.BatchNorm1d(2 * dim)\n        self.dropout = nn.Dropout(dropout)\n        self.residual_dense = nn.ModuleList()\n        for i in range(res_blocks):\n            self.residual_dense.append(\n                ResidualDense(2 * dim, dropout, activation, random_seed) # Pass 2*dim here\n                )\n        self.output = nn.Linear(2 * dim, 1) # Output layer takes 2*dim input\n\n    def forward(self, x, mask=None):\n        # x shape: [batch_size, dim, seq_len]\n        # mask shape: [batch_size, seq_len]\n        x_pooled, weights = self.light_attention(x, mask) # x_pooled shape: [batch_size, 2 * dim]\n        x = self.batchnorm(x_pooled)\n        x = self.dropout(x)\n        hidden_embedding = x # Store embedding before output layer if needed\n        for layer in self.residual_dense:\n            hidden_embedding = layer(hidden_embedding) # Pass through residual blocks\n\n        y = self.output(hidden_embedding) # Final prediction shape: [batch_size, 1]\n        # No need to flatten if output is already [batch_size, 1]\n        # y = y.flatten() # Use flatten() only if shape is different, e.g. [batch_size]\n\n        # Return in the expected list format [prediction, final_hidden_embedding, attention_weights]\n        return [y, hidden_embedding, weights]\n\nprint(\"RLAT Components Defined.\")\n\n# --- Utility Functions from trainutils.py ---\n# (Keep the label_distribution_smoothing, get_sample_weights, performance functions here as before)\n# ... (paste trainutils functions here again) ...\n\ndef label_distribution_smoothing(y, bins=None, ks=5, sigma=2, normalize=True):\n    \"\"\"\n    Return a smoothed label distribution derived by convolving a symetric kernel\n    to the empirical label distribution. If bins is None, split the data (y) into bins\n    such that each bin corresponds to 1.0 pH unit. Otherwise if bins is an integer, split\n    bins into as many bins as is specified.\n    See the paper,\n    Yang, Zha, Chen, et al, 2021. Delving into deep imbalanced regression.\n    Code adapted from https://github.com/YyzHarry/imbalanced-regression\n    \"\"\"\n    y = np.asarray(y)\n    if y.size == 0: # Handle empty input\n        return np.array([])\n\n    min_y, max_y = np.min(y), np.max(y)\n\n    if bins is None:\n        bins = int(np.ceil(max_y - min_y)) + 1 # Ensure bins cover range\n        bins = max(1, bins) # Ensure at least one bin\n    else:\n        bins = max(1, int(bins)) # Ensure positive integer\n\n    # Ensure range covers all values, handle constant case\n    if min_y == max_y:\n        range_min = min_y - 0.5\n        range_max = max_y + 0.5\n    else:\n        range_min, range_max = min_y, max_y\n\n    bin_freqs, bin_borders = np.histogram(y, range=(range_min, range_max), bins=bins)\n\n    # Find bin index for each y value using searchsorted\n    bin_indices = np.searchsorted(bin_borders[1:-1], y, side='right')\n\n    # Compute kernel window\n    if ks <= 0:\n         kernel_window = np.array([1.0])\n    else:\n        half_ks = (ks - 1) // 2\n        base_kernel = np.array([0.] * half_ks + [1.] + [0.] * half_ks)\n        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma)\n        # Normalize kernel sum to 1\n        if np.sum(kernel_window) > 0:\n            kernel_window /= np.sum(kernel_window)\n        else: # Handle case where sigma is very large, kernel becomes flat\n             kernel_window = np.ones_like(base_kernel) / len(base_kernel)\n\n\n    # Derive Kernel estimation using convolution\n    bin_kde = convolve1d(np.array(bin_freqs, dtype=float), weights=kernel_window, mode='constant', cval=0.0)\n\n    # Map KDE values back to original samples\n    epsilon = 1e-12 # Avoid division by zero\n    y_kde = np.array([bin_kde[idx] for idx in bin_indices]) + epsilon\n\n    # Normalize KDE so minimum density corresponds to weight 1 before inversion\n    min_kde_val = np.min(y_kde)\n    if normalize and min_kde_val > 0:\n        y_kde = y_kde / min_kde_val\n    elif normalize:\n        print(\"Warning: Minimum KDE value is zero or negative, skipping normalization.\")\n\n    return y_kde\n\ndef get_sample_weights(ydata, method='bin_inv', bin_borders=[5,9]):\n    \"\"\"\n    Return an array of sample weights computed with different methods.\n    \"\"\"\n    assert method in ['None', 'bin_inv', 'bin_inv_sqrt', 'LDS_inv', 'LDS_inv_sqrt',\n                      'LDS_extreme', None]\n\n    ydata = np.asarray(ydata)\n    if ydata.size == 0:\n        return np.array([])\n    weights = np.ones(len(ydata))\n\n    if method == 'None' or method is None:\n        pass\n\n    elif method in ['bin_inv', 'bin_inv_sqrt']:\n        y_binned = np.digitize(ydata, bin_borders)\n        bin_class, bin_freqs = np.unique(y_binned, return_counts=True)\n        inv_freq_dict = dict(zip(bin_class, 1 / (bin_freqs + 1e-9)))\n        weights = np.array([inv_freq_dict.get(value, 1.0) for value in y_binned]) # Use .get for safety\n\n    elif method in ['LDS_inv', 'LDS_inv_sqrt', 'LDS_extreme']:\n        effdist = label_distribution_smoothing(ydata, bins=100, ks=5, sigma=2)\n        weights = 1.0 / effdist\n        if method == 'LDS_extreme':\n            relevance = np.logical_or(ydata <= bin_borders[0], ydata >= bin_borders[-1]).astype(float)\n            relevance = relevance * (1 - 0.5) + 0.5\n            weights = weights * relevance\n\n    if method in ['bin_inv_sqrt', 'LDS_inv_sqrt']:\n        # Ensure non-negative before sqrt\n        weights = np.sqrt(np.maximum(weights, 0))\n\n    # Normalize so weights have a mean of 1, handle potential NaN/inf/zero mean\n    weights[~np.isfinite(weights)] = 0.0 # Set non-finite weights to 0\n    mean_weight = np.mean(weights)\n    if mean_weight > 1e-9:\n         weights = weights / mean_weight\n    else:\n        print(\"Warning: Mean of weights is close to zero, using equal weights.\")\n        weights = np.ones(len(ydata)) # Fallback\n\n    return weights\n\n\ndef performance(ytrue, ypred, weights=None, bins=[5,9]):\n    '''Return a dictionary of performance metrics evaluated on predictions'''\n    perf = {}\n    ytrue, ypred = np.asarray(ytrue), np.asarray(ypred)\n\n    if ytrue.size == 0 or ypred.size == 0: # Handle empty inputs\n        return {'rmse': float('inf'), 'r2': 0.0, 'rho': 0.0, 'r': 0.0, 'mcc': 0.0, 'f1score': 0.0, 'auc': 0.0}\n\n\n    if weights is None:\n        weights = np.ones_like(ytrue)\n    weights = np.asarray(weights)\n    weights[~np.isfinite(weights)] = 0.0\n    weights = np.maximum(weights, 0.0)\n\n    # Filter out samples where true value is NaN if any\n    valid_idx = ~np.isnan(ytrue)\n    ytrue = ytrue[valid_idx]\n    ypred = ypred[valid_idx]\n    weights = weights[valid_idx]\n\n    if ytrue.size == 0: # Handle case where all true values were NaN\n         return {'rmse': float('inf'), 'r2': 0.0, 'rho': 0.0, 'r': 0.0, 'mcc': 0.0, 'f1score': 0.0, 'auc': 0.0}\n\n    # Replace NaN predictions with mean of true values for metric calculation\n    ypred_mean = np.nanmean(ytrue) # Mean of valid true values\n    ypred = np.nan_to_num(ypred, nan=ypred_mean)\n\n\n    # Normalize weights if sum is positive\n    sum_weights = np.sum(weights)\n    if sum_weights <= 1e-9:\n        print(\"Warning: Sum of weights is zero in performance calculation. Using equal weights.\")\n        weights = np.ones_like(weights) / len(weights)\n\n\n    # Correlation\n    try:\n        perf['rho'] = float(spearmanr(ytrue, ypred)[0]) if len(np.unique(ytrue)) > 1 and len(np.unique(ypred)) > 1 else 0.0\n        perf['r'] = float(pearsonr(ytrue, ypred)[0]) if len(np.unique(ytrue)) > 1 and len(np.unique(ypred)) > 1 else 0.0\n    except ValueError:\n        perf['rho'] = 0.0\n        perf['r'] = 0.0\n\n    # Sample-weighted Regression Metrics\n    try:\n        perf['rmse'] = float(np.sqrt(metrics.mean_squared_error(ytrue, ypred, sample_weight=weights)))\n        perf['r2'] = float(metrics.r2_score(ytrue, ypred, sample_weight=weights))\n    except ValueError:\n        perf['rmse'] = float('inf')\n        perf['r2'] = 0.0\n\n    # Classification performance of binned data\n    try:\n        ytrue_binned = np.digitize(ytrue, bins)\n        ypred_binned = np.digitize(ypred, bins)\n        present_classes = sorted(np.unique(ytrue_binned))\n\n        if len(present_classes) > 1 : # Need multiple classes for MCC, F1, AUC\n             perf['mcc'] = float(metrics.matthews_corrcoef(ytrue_binned, ypred_binned, sample_weight=weights))\n             f1score = float(metrics.f1_score(ytrue_binned, ypred_binned, sample_weight=weights, average='weighted', zero_division=0))\n             # AUC requires scores, not direct multi-class calculation from binary predictions this way\n             # Using one-vs-rest approach\n             auc_scores = []\n             for cls in present_classes:\n                 ytrue_cls = (ytrue_binned == cls).astype(int)\n                 ypred_cls = (ypred_binned == cls).astype(int)\n                 if len(np.unique(ytrue_cls)) > 1: # Check if class is present\n                     auc_scores.append(metrics.roc_auc_score(ytrue_cls, ypred_cls, sample_weight=weights))\n                 else:\n                      auc_scores.append(0.5) # Assign neutral score if only one class present\n             perf['auc'] = np.mean(auc_scores) if auc_scores else 0.0\n             perf['f1score'] = f1score\n\n        else: # Only one class present\n            perf['mcc'] = 0.0\n            perf['f1score'] = 0.0 # Or 1.0 if predictions are perfect? Usually 0 for single class.\n            perf['auc'] = 0.5 # Undefined, often set to 0.5\n\n\n    except ValueError as e:\n        print(f\"Error calculating classification metrics: {e}\")\n        perf['mcc'] = 0.0\n        perf['f1score'] = 0.0\n        perf['auc'] = 0.0\n\n    return perf\n\nprint(\"Training Utilities Defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.298918Z","iopub.execute_input":"2025-04-24T02:02:50.299605Z","iopub.status.idle":"2025-04-24T02:02:50.334210Z","shell.execute_reply.started":"2025-04-24T02:02:50.299575Z","shell.execute_reply":"2025-04-24T02:02:50.333452Z"}},"outputs":[{"name":"stdout","text":"RLAT Components Defined.\nTraining Utilities Defined.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# =============================================================================\n# 5. Combined Model Definition Cell (from combined_model.py)\n# =============================================================================\n# Import RLAT from the ephod package IF INSTALLED, otherwise paste definition in Cell 4\n# Option 1: If ephod package is installed or accessible in Kaggle env\n# try:\n#     from ephod.training.nn_models import ResidualLightAttention\n#     print(\"Imported ResidualLightAttention from ephod package.\")\n# except ImportError:\n#     print(\"Could not import from ephod package. Make sure RLAT is defined in Cell 4.\")\n#     # Fallback to placeholder if needed - requires pasting the RLAT code in cell 4\n#     if 'ResidualLightAttention' not in globals():\n#          ResidualLightAttention = # ... Define placeholder or raise error ...\n\n# Option 2: Assume RLAT code was pasted into Cell 4 (RECOMMENDED FOR NOTEBOOK)\n# Make sure the class 'ResidualLightAttention' is defined above in Cell 4.\nif 'ResidualLightAttention' not in globals():\n    raise NameError(\"ResidualLightAttention class definition not found. Paste it into Cell 4.\")\nelse:\n    print(\"Found ResidualLightAttention definition.\")\n\n\nclass SequenceTopHModel(nn.Module):\n    \"\"\"\n    Combined model: LSTM feature extractor followed by RLAT for pH prediction.\n    Designed to be trained end-to-end.\n    \"\"\"\n    def __init__(self,\n                 # LSTM args\n                 vocab_size=VOCAB_SIZE, # Use global vocab size\n                 lstm_embedding_dim=LSTM_EMB_DIM,\n                 lstm_hidden_dim=LSTM_HIDDEN_DIM,\n                 num_lstm_layers=LSTM_LAYERS,\n                 lstm_dropout=LSTM_DROPOUT,\n                 # RLAT args (must match LSTM output)\n                 rlat_kernel_size=RLAT_KERNEL_SIZE,\n                 rlat_dropout=RLAT_DROPOUT,\n                 rlat_activation=RLAT_ACTIVATION,\n                 rlat_res_blocks=RLAT_RES_BLOCKS,\n                 random_seed=RANDOM_SEED):\n        super().__init__()\n        _ = torch.manual_seed(random_seed)\n\n        # 1. LSTM Feature Extractor\n        self.lstm_extractor = LSTMFeatureExtractor(\n            vocab_size=vocab_size,\n            embedding_dim=lstm_embedding_dim,\n            lstm_hidden_dim=lstm_hidden_dim,\n            num_lstm_layers=num_lstm_layers,\n            dropout=lstm_dropout,\n            random_seed=random_seed\n        )\n        lstm_output_dim = self.lstm_extractor.output_dim # Get actual output dim (e.g., 512)\n\n        # 2. RLAT Head\n        # CRITICAL: Initialize RLAT with the output dimension of the LSTM\n        self.rlat_head = ResidualLightAttention(\n            dim=lstm_output_dim, # Match LSTM output!\n            kernel_size=rlat_kernel_size,\n            dropout=rlat_dropout,\n            activation=rlat_activation,\n            res_blocks=rlat_res_blocks,\n            random_seed=random_seed # Pass seed if RLAT accepts it\n        )\n\n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Args:\n            input_ids (torch.Tensor): Batch of token IDs [batch_size, seq_len]\n            attention_mask (torch.Tensor): Batch of masks [batch_size, seq_len] (1=real, 0=pad)\n\n        Returns:\n            torch.Tensor: Predicted pH values [batch_size]\n        \"\"\"\n        # 1. Get LSTM features\n        # Input: [batch_size, seq_len]\n        # Output: [batch_size, lstm_output_dim, seq_len]\n        lstm_features = self.lstm_extractor(input_ids)\n\n        # 2. Predict pH using RLAT head\n        # Input: [batch_size, lstm_output_dim, seq_len], [batch_size, seq_len]\n        # Output list: [y_pred, hidden_embedding, attention_weights] (Ensure RLAT returns list)\n        # The mask needs to be boolean or float depending on RLAT implementation\n        # Assuming mask needs to be [batch_size, seq_len]\n        rlat_output = self.rlat_head(lstm_features, attention_mask.bool()) # Pass mask\n        y_pred = rlat_output[0].squeeze(-1) # Get the final pH prediction and remove trailing dim if any\n\n        return y_pred # Only return prediction for training loss\n\n    def get_num_params(self):\n        \"\"\"Helper to count parameters.\"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n\nprint(\"Combined LSTM-RLAT Model Defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.335394Z","iopub.execute_input":"2025-04-24T02:02:50.335659Z","iopub.status.idle":"2025-04-24T02:02:50.353007Z","shell.execute_reply.started":"2025-04-24T02:02:50.335637Z","shell.execute_reply":"2025-04-24T02:02:50.352454Z"}},"outputs":[{"name":"stdout","text":"Found ResidualLightAttention definition.\nCombined LSTM-RLAT Model Defined.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\n# =============================================================================\n# 6. Dataset & Collate Function Cell\n# =============================================================================\nclass SequencepHDataset(Dataset):\n    def __init__(self, dataframe, sample_weight_method=SAMPLE_WEIGHT_METHOD): # Use config\n        if not isinstance(dataframe, pd.DataFrame):\n             raise ValueError(\"Input 'dataframe' must be a pandas DataFrame.\")\n        if 'Sequence' not in dataframe.columns or 'pHopt' not in dataframe.columns:\n             raise ValueError(\"DataFrame must contain 'Sequence' and 'pHopt' columns.\")\n\n        self.sequences = dataframe['Sequence'].values\n        self.labels = dataframe['pHopt'].values.astype(np.float32)\n        # Store accessions if present, otherwise generate dummy ones\n        self.accessions = dataframe['Accession'].values if 'Accession' in dataframe.columns else [f\"Seq_{i}\" for i in range(len(dataframe))]\n\n\n        # Pre-calculate sample weights\n        if sample_weight_method != 'None' and sample_weight_method is not None:\n            print(f\"Calculating sample weights using method: {sample_weight_method}\")\n            # Ensure labels are passed correctly to weighting function\n            self.weights = get_sample_weights(self.labels, method=sample_weight_method)\n            self.weights = self.weights.astype(np.float32)\n        else:\n            print(\"No sample weighting applied.\")\n            self.weights = np.ones_like(self.labels, dtype=np.float32)\n\n        print(f\"Dataset created with {len(self.sequences)} samples. Weight method: {sample_weight_method}\")\n        if len(self.sequences) == 0:\n            print(\"Warning: Dataset is empty!\")\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        if idx >= len(self.sequences):\n             raise IndexError(\"Index out of bounds\")\n        seq = self.sequences[idx]\n        label = self.labels[idx]\n        weight = self.weights[idx]\n\n        if not isinstance(seq, str):\n             print(f\"Warning: Sequence at index {idx} is not a string: {seq}. Attempting conversion.\")\n             seq = str(seq)\n\n        # Tokenize the sequence here\n        token_ids = tokenize_sequence(seq)\n        return token_ids, label, weight\n\ndef collate_fn(batch):\n    \"\"\"Collates data samples into batches.\"\"\"\n    # Filter out potential None values if dataset loading had issues\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        return None, None, None, None # Return None if batch is empty\n\n    token_ids_list, labels, weights = zip(*batch)\n\n    # Pad sequences in the batch - Use the function defined in Cell 3\n    padded_ids, masks, _ = pad_sequences(token_ids_list, max_len=1024, pad_value=PAD_ID)\n\n    # Convert labels and weights to tensors\n    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n\n    # Ensure weights are positive and finite\n    weights_tensor = torch.clamp(weights_tensor, min=0.0)\n    weights_tensor[torch.isnan(weights_tensor) | torch.isinf(weights_tensor)] = 0.0\n\n\n    return padded_ids, masks, labels_tensor, weights_tensor\n\nprint(\"Dataset and Collate Function Defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.353784Z","iopub.execute_input":"2025-04-24T02:02:50.353998Z","iopub.status.idle":"2025-04-24T02:02:50.373393Z","shell.execute_reply.started":"2025-04-24T02:02:50.353983Z","shell.execute_reply":"2025-04-24T02:02:50.372689Z"}},"outputs":[{"name":"stdout","text":"Dataset and Collate Function Defined.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# =============================================================================\n# 7. Loss Function Cell\n# =============================================================================\ndef weighted_rmse_loss(y_pred, y_true, weight):\n    \"\"\"Calculates the weighted root mean squared error.\"\"\"\n    if y_pred.shape != y_true.shape:\n        y_pred = y_pred.squeeze() # Try to fix shape mismatch\n    if y_pred.shape != y_true.shape:\n         raise ValueError(f\"Shape mismatch: y_pred {y_pred.shape}, y_true {y_true.shape}\")\n    if y_pred.shape != weight.shape:\n        # Try broadcasting weight if it's [batch_size] and prediction is [batch_size, 1] or vice versa\n         if weight.shape[0] == y_pred.shape[0] and len(weight.shape)==1:\n              weight = weight.view(-1, 1) # Reshape weight to match prediction if needed\n              if weight.shape[0] != y_pred.shape[0]: # Check again\n                  raise ValueError(f\"Shape mismatch after reshape: y_pred {y_pred.shape}, weight {weight.shape}\")\n         elif y_pred.shape[0] == weight.shape[0] and len(y_pred.shape)==1:\n             y_pred = y_pred.view(-1,1) # Reshape prediction to match weight\n             if y_pred.shape[0] != weight.shape[0]: # Check again\n                 raise ValueError(f\"Shape mismatch after reshape: y_pred {y_pred.shape}, weight {weight.shape}\")\n         else:\n              raise ValueError(f\"Shape mismatch: y_pred {y_pred.shape}, weight {weight.shape}\")\n\n\n    loss = torch.mean(((y_pred - y_true) ** 2) * weight)\n    # Add epsilon to prevent sqrt(0) and potential NaN gradients\n    return torch.sqrt(loss + 1e-9)\n\nprint(\"Loss Function Defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.381040Z","iopub.execute_input":"2025-04-24T02:02:50.381263Z","iopub.status.idle":"2025-04-24T02:02:50.390618Z","shell.execute_reply.started":"2025-04-24T02:02:50.381247Z","shell.execute_reply":"2025-04-24T02:02:50.389887Z"}},"outputs":[{"name":"stdout","text":"Loss Function Defined.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# =============================================================================\n# 8. Train/Validation Loop Cell\n# =============================================================================\ndef train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n    model.train()\n    total_loss = 0.0\n    num_batches = len(dataloader)\n    if num_batches == 0:\n        print(\"Warning: Training dataloader is empty.\")\n        return 0.0\n\n    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n        # Check if collate_fn returned None (empty batch)\n        if batch_data[0] is None:\n            print(f\"Skipping empty batch {batch_idx+1}\")\n            continue\n\n        ids, masks, labels, weights = batch_data\n        ids, masks, labels, weights = ids.to(device), masks.to(device), labels.to(device), weights.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(ids, masks) # Get predictions [batch_size]\n\n        # Ensure labels have the same shape as outputs for loss calculation\n        if outputs.shape != labels.shape:\n            labels = labels.view(outputs.shape) # Reshape labels if necessary\n\n        loss = loss_fn(outputs, labels, weights)\n\n        # Handle potential NaN loss\n        if torch.isnan(loss):\n            print(f\"Warning: NaN loss detected at batch {batch_idx}. Skipping batch.\")\n            # Optionally: Investigate inputs/outputs/weights for this batch\n            # print(\"NaN Debug Info:\")\n            # print(\"IDs:\", ids)\n            # print(\"Masks:\", masks)\n            # print(\"Labels:\", labels)\n            # print(\"Weights:\", weights)\n            # print(\"Outputs:\", outputs)\n            continue # Skip backward pass and optimizer step for this batch\n\n        loss.backward()\n        # Optional: Gradient clipping\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / num_batches if num_batches > 0 else 0.0\n\n\ndef validate_epoch(model, dataloader, loss_fn, device):\n    model.eval()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n    all_weights = [] # Use for calculating weighted metrics if needed\n    num_batches = len(dataloader)\n\n    if num_batches == 0:\n        print(\"Warning: Validation dataloader is empty.\")\n        return 0.0, {}\n\n    with torch.no_grad():\n        for batch_data in tqdm(dataloader, desc=\"Validation\", leave=False):\n             if batch_data[0] is None: # Skip empty batches\n                 continue\n             ids, masks, labels, weights = batch_data\n             ids, masks, labels, weights = ids.to(device), masks.to(device), labels.to(device), weights.to(device)\n\n             outputs = model(ids, masks) # [batch_size]\n\n             if outputs.shape != labels.shape:\n                 labels = labels.view(outputs.shape)\n\n             loss = loss_fn(outputs, labels, weights) # Use the same weighted loss for consistency, or unweighted RMSE for simple val loss\n             if not torch.isnan(loss): # Only add valid loss values\n                 total_loss += loss.item()\n\n             all_preds.extend(outputs.cpu().numpy())\n             all_labels.extend(labels.cpu().numpy())\n             all_weights.extend(weights.cpu().numpy()) # Collect weights if using trainutils.performance\n\n    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n\n    # Calculate performance metrics using trainutils\n    # Use standard 'bin_inv' weighting for evaluation metrics for comparability, as in original paper\n    # Or pass None to use unweighted metrics\n    try:\n        # Ensure labels and preds are numpy arrays\n        all_labels_np = np.array(all_labels)\n        all_preds_np = np.array(all_preds)\n        # Calculate evaluation weights based on the true validation labels\n        eval_weights = get_sample_weights(all_labels_np, method='bin_inv')\n        perf_metrics = performance(all_labels_np, all_preds_np, weights=eval_weights)\n    except Exception as e:\n        print(f\"Error during performance calculation: {e}\")\n        # Return default empty metrics\n        perf_metrics = {'rmse': float('inf'), 'r2': 0.0, 'rho': 0.0, 'r': 0.0, 'mcc': 0.0, 'f1score': 0.0, 'auc': 0.0}\n\n\n    return avg_loss, perf_metrics\n\n\nprint(\"Train/Validation Functions Defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.544802Z","iopub.execute_input":"2025-04-24T02:02:50.545064Z","iopub.status.idle":"2025-04-24T02:02:50.556922Z","shell.execute_reply.started":"2025-04-24T02:02:50.545043Z","shell.execute_reply":"2025-04-24T02:02:50.556048Z"}},"outputs":[{"name":"stdout","text":"Train/Validation Functions Defined.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# # %% [code]\n# # =============================================================================\n# # 9. Main Training Execution Cell (Verified Saving Logic)\n# # =============================================================================\n\n# print(\"--- Starting LSTM-RLAT Training ---\")\n\n# # --- Device Configuration ---\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # --- Set Random Seed ---\n# torch.manual_seed(RANDOM_SEED)\n# np.random.seed(RANDOM_SEED)\n# if device == torch.device(\"cuda\"):\n#     torch.cuda.manual_seed_all(RANDOM_SEED)\n#     # Optional: For full reproducibility, may need these, but can slow down training\n#     # torch.backends.cudnn.deterministic = True\n#     # torch.backends.cudnn.benchmark = False\n\n# # --- Load Data ---\n# try:\n#     print(f\"Loading data from: {TARGET_DATA_PATH}\")\n#     if not os.path.exists(TARGET_DATA_PATH):\n#          raise FileNotFoundError(f\"Data file not found at {TARGET_DATA_PATH}. Please check path and ensure dataset is uploaded.\")\n\n#     full_df = pd.read_csv(TARGET_DATA_PATH, index_col=None) # Adjust index_col if needed\n#     if 'Split' not in full_df.columns:\n#          raise ValueError(\"CSV file must contain a 'Split' column with values like 'Training', 'Validation', 'Testing'.\")\n\n#     # Filter for train/validation splits\n#     train_df = full_df[full_df['Split'] == 'Training'].reset_index(drop=True)\n#     val_df = full_df[full_df['Split'] == 'Validation'].reset_index(drop=True)\n#     test_df = full_df[full_df['Split'] == 'Testing'].reset_index(drop=True) # Load test data for final eval size check\n\n#     print(f\"Loaded {len(full_df)} total entries.\")\n#     print(f\"Training set size: {len(train_df)}\")\n#     print(f\"Validation set size: {len(val_df)}\")\n#     print(f\"Test set size: {len(test_df)}\")\n\n#     if len(train_df) == 0 or len(val_df) == 0:\n#          raise ValueError(\"Training or Validation split is empty. Check 'Split' column in CSV.\")\n\n# except FileNotFoundError as e:\n#     print(f\"ERROR: {e}\")\n#     print(\"Please ensure your data is uploaded to Kaggle and the TARGET_DATA_PATH variable is set correctly.\")\n#     # Stop execution if data isn't loaded\n#     # In a notebook, you might just print the error and let the user fix it.\n#     # For automatic runs, use sys.exit(1)\n#     sys.exit(1) # Or comment out to allow fixing path and re-running cell\n# except Exception as e:\n#     print(f\"An unexpected error occurred loading data: {e}\")\n#     sys.exit(1) # Or comment out\n\n# # --- Create Datasets and Dataloaders ---\n# try:\n#     train_dataset = SequencepHDataset(train_df, sample_weight_method=SAMPLE_WEIGHT_METHOD)\n#     # No sample weighting needed for calculating validation loss itself,\n#     # but eval metrics will use weighted calculation inside validate_epoch\n#     val_dataset = SequencepHDataset(val_df, sample_weight_method='None')\n\n#     if len(train_dataset) == 0 or len(val_dataset) == 0:\n#         raise ValueError(\"Dataset creation resulted in empty dataset(s).\")\n\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True) # drop_last=True can help with batchnorm issues on last batch\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n\n#     print(f\"Train DataLoader: {len(train_loader)} batches\")\n#     print(f\"Validation DataLoader: {len(val_loader)} batches\")\n\n# except Exception as e:\n#     print(f\"Error creating Datasets or DataLoaders: {e}\")\n#     sys.exit(1) # Or comment out\n\n\n# # --- Build Model ---\n# model = SequenceTopHModel(\n#     vocab_size=VOCAB_SIZE,\n#     lstm_embedding_dim=LSTM_EMB_DIM,\n#     lstm_hidden_dim=LSTM_HIDDEN_DIM,\n#     num_lstm_layers=LSTM_LAYERS,\n#     lstm_dropout=LSTM_DROPOUT,\n#     rlat_kernel_size=RLAT_KERNEL_SIZE,\n#     rlat_dropout=RLAT_DROPOUT,\n#     rlat_res_blocks=RLAT_RES_BLOCKS,\n#     rlat_activation=RLAT_ACTIVATION,\n#     random_seed=RANDOM_SEED\n# ).to(device)\n# print(f\"Model created with {model.get_num_params():,} parameters.\")\n# # Optional: Print model summary\n# # print(model)\n\n\n# # --- Optimizer and Loss ---\n# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)\n# loss_fn = weighted_rmse_loss # Use the function defined earlier\n# # Scheduler monitors validation loss\n# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=REDUCE_LR_PATIENCE, verbose=True, min_lr=1e-7) # Added min_lr\n\n\n# # --- Training Loop Variables ---\n# best_val_loss = float('inf') # Initialize best loss to infinity\n# epochs_no_improve = 0\n# training_log = [] # Store epoch results\n\n# # --- Training Loop ---\n# print(\"\\n--- Starting Training Loop ---\")\n# start_time_total = time.time()\n\n# # Initialize epoch variable outside loop for final message\n# epoch = 0\n\n# for epoch in range(1, EPOCHS + 1):\n#     start_time_epoch = time.time()\n#     print(f\"\\n--- Epoch {epoch}/{EPOCHS} ---\")\n\n#     # Ensure model is in training mode\n#     model.train()\n#     train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n\n#     # Ensure model is in evaluation mode for validation\n#     model.eval()\n#     # Get validation loss AND metrics dictionary\n#     current_val_loss, val_metrics = validate_epoch(model, val_loader, loss_fn, device)\n\n#     # Handle potential NaN in validation loss before using it\n#     if np.isnan(current_val_loss):\n#         print(\"Warning: Validation loss is NaN. Skipping scheduler step and model saving for this epoch.\")\n#         # Optionally stop training if validation becomes unstable\n#         # epochs_no_improve += 1 # Consider if NaN counts as non-improvement\n#     else:\n#         # Scheduler steps based on the valid validation loss\n#         scheduler.step(current_val_loss)\n\n#         # --- Save model and handle early stopping BASED ON VALIDATION LOSS ---\n#         if current_val_loss < best_val_loss:\n#             print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {current_val_loss:.4f}). Saving model...\")\n#             best_val_loss = current_val_loss # Update best loss\n#             epochs_no_improve = 0\n#             # Save the best model\n#             save_path = os.path.join(SAVEDIR, f\"{MODEL_NAME}_best.pt\")\n#             # Ensure model is on CPU before saving to avoid potential GPU memory issues during saving\n#             # model.to('cpu') # Move model to CPU temporarily\n#             torch.save({\n#                 'epoch': epoch,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#                 'best_val_loss': best_val_loss, # Save the best loss value\n#                 'val_metrics_at_best': val_metrics, # Optionally save metrics dict at best epoch\n#                 'config': { # Save config used for this run\n#                      'lstm_emb_dim': LSTM_EMB_DIM, 'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n#                      'lstm_layers': LSTM_LAYERS, 'lstm_dropout': LSTM_DROPOUT,\n#                      'rlat_kernel_size': RLAT_KERNEL_SIZE, 'rlat_dropout': RLAT_DROPOUT,\n#                      'rlat_res_blocks': RLAT_RES_BLOCKS, 'rlat_activation': RLAT_ACTIVATION,\n#                      'learning_rate': LEARNING_RATE, 'l2_reg': L2_REG, 'batch_size': BATCH_SIZE,\n#                      'sample_weight_method': SAMPLE_WEIGHT_METHOD, 'random_seed': RANDOM_SEED\n#                 }\n#             }, save_path)\n#             # model.to(device) # Move model back to original device\n#         else:\n#             epochs_no_improve += 1\n#             print(f\"  Validation loss did not improve for {epochs_no_improve} epoch(s). Best: {best_val_loss:.4f}\")\n\n#     # Print Epoch Summary (regardless of NaN status, report metrics if available)\n#     epoch_duration = time.time() - start_time_epoch\n#     print(f\"Epoch {epoch} Summary:\")\n#     print(f\"  Train Loss: {train_loss:.4f}\")\n#     print(f\"  Val Loss:   {current_val_loss:.4f}\") # Report current loss, even if NaN\n#     print(f\"  Val RMSE:   {val_metrics.get('rmse', float('nan')):.4f}\")\n#     print(f\"  Val R2:     {val_metrics.get('r2', float('nan')):.4f}\")\n#     print(f\"  Val Rho:    {val_metrics.get('rho', float('nan')):.4f}\")\n#     print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n#     print(f\"  Duration:   {epoch_duration:.2f}s\")\n\n#     # Log results\n#     log_entry = {\n#         'epoch': epoch,\n#         'train_loss': train_loss,\n#         'val_loss': current_val_loss, # Log current val loss\n#         'val_rmse': val_metrics.get('rmse', float('nan')),\n#         'val_r2': val_metrics.get('r2', float('nan')),\n#         'val_rho': val_metrics.get('rho', float('nan')),\n#         'val_r': val_metrics.get('r', float('nan')),\n#         'val_mcc': val_metrics.get('mcc', float('nan')),\n#         'val_f1score': val_metrics.get('f1score', float('nan')),\n#         'val_auc': val_metrics.get('auc', float('nan')),\n#         'learning_rate': optimizer.param_groups[0]['lr'],\n#         'duration': epoch_duration\n#     }\n#     training_log.append(log_entry)\n\n\n#     # Check for early stopping\n#     if epochs_no_improve >= STOP_PATIENCE:\n#         print(f\"\\nEarly stopping triggered after {STOP_PATIENCE} epochs without improvement on validation loss.\")\n#         break\n#     # Optional: Stop if validation loss becomes NaN\n#     # if np.isnan(current_val_loss):\n#     #    print(\"\\nStopping training due to NaN validation loss.\")\n#     #    break\n\n\n# # --- End of Training ---\n# total_duration = time.time() - start_time_total\n# print(f\"\\n--- Training Finished ---\")\n# # Ensure epoch reports the actual last epoch number if early stopping occurred\n# print(f\"Training finished after {epoch} epochs.\")\n# print(f\"Total training time: {total_duration/60:.2f} minutes.\")\n# print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n\n# # Save the final training log\n# log_df = pd.DataFrame(training_log)\n# log_path = os.path.join(SAVEDIR, f\"{MODEL_NAME}_training_log.csv\")\n# log_df.to_csv(log_path, index=False)\n# print(f\"Training log saved to {log_path}\")\n\n# print(\"\\n --- LSTM-RLAT Training Script Completed ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.558524Z","iopub.execute_input":"2025-04-24T02:02:50.558714Z","iopub.status.idle":"2025-04-24T02:02:50.576581Z","shell.execute_reply.started":"2025-04-24T02:02:50.558700Z","shell.execute_reply":"2025-04-24T02:02:50.575893Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# %% [code]\n# =============================================================================\n# 9. Optuna Hyperparameter Optimization Setup (Corrected & Tuned Ranges)\n# =============================================================================\n!pip install optuna -q # Install optuna quietly\nimport optuna\nimport gc # Garbage collector\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport time\nimport os\nimport sys\nfrom tqdm.notebook import tqdm # Ensure notebook tqdm is used\n\n# --- Ensure Previous Cells (1-8) containing necessary definitions are executed ---\n# Includes: LSTM Module, RLAT Components, Combined Model, Dataset, Loss, Train/Val Funcs\n\nprint(\"--- Setting up Optuna Hyperparameter Optimization (Tuned Ranges) ---\")\n\n# --- Device Configuration ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Load Data ONCE outside the objective function ---\n# (Assuming train_df, val_df defined from previous cell execution)\nif 'train_df' not in globals() or 'val_df' not in globals():\n    print(\"ERROR: train_df or val_df not defined. Please execute data loading cell first.\")\n    # Add data loading code here again if necessary, or stop\n    import sys\n    sys.exit(1)\nelse:\n    print(f\"Using pre-loaded Training ({len(train_df)}) and Validation ({len(val_df)}) data.\")\n\n\n# --- Define the Objective Function for Optuna ---\n\ndef objective(trial):\n    \"\"\"Optuna objective function for hyperparameter tuning.\"\"\"\n    global train_df, val_df # Access dataframes defined outside\n\n    # --- 1. Suggest Hyperparameters (with adjusted ranges) ---\n    cfg = {\n        # LSTM Args\n        'lstm_emb_dim': trial.suggest_categorical('lstm_emb_dim', [128, 256]), # Kept embedding dim reasonable\n        'lstm_hidden_dim': trial.suggest_categorical('lstm_hidden_dim', [128, 256]), # Reduced max hidden dim\n        'lstm_layers': trial.suggest_int('lstm_layers', 1, 2), # Reduced max layers\n        'lstm_dropout': trial.suggest_float('lstm_dropout', 0.1, 0.4, step=0.1), # Slightly reduced max dropout\n        # RLAT Args\n        'rlat_kernel_size': trial.suggest_categorical('rlat_kernel_size', [5, 7, 9]), # Removed smallest kernel\n        'rlat_dropout': trial.suggest_float('rlat_dropout', 0.1, 0.5, step=0.1), # Slightly reduced max dropout\n        'rlat_res_blocks': trial.suggest_int('rlat_res_blocks', 2, 4), # Reduced max res blocks\n        'rlat_activation': trial.suggest_categorical('rlat_activation', ['relu', 'elu', 'gelu']),\n        # Training Args\n        'learning_rate': trial.suggest_float('learning_rate', 5e-5, 8e-4, log=True), # Adjusted LR range slightly\n        'l2_reg': trial.suggest_float('l2_reg', 1e-6, 5e-5, log=True), # Adjusted reg range slightly\n        'batch_size': trial.suggest_categorical('batch_size', [32, 64]), # Increased min batch size\n        'sample_weight_method': trial.suggest_categorical('sample_weight_method', ['None', 'bin_inv_sqrt', 'LDS_inv_sqrt']),\n        'optimizer_name': trial.suggest_categorical('optimizer_name', ['Adam', 'AdamW']),\n    }\n    print(f\"\\n--- Starting Trial {trial.number} ---\")\n    print(f\"Parameters: {cfg}\")\n\n    # --- 2. Setup Trial (Dataset, Model, Optimizer) ---\n    trial_seed = RANDOM_SEED # Use same base seed for better comparison across trials initially\n    torch.manual_seed(trial_seed)\n    np.random.seed(trial_seed)\n    if device == torch.device(\"cuda\"):\n        torch.cuda.manual_seed_all(trial_seed)\n\n    try:\n        # Create datasets using the suggested weight method for training data\n        current_train_dataset = SequencepHDataset(train_df, sample_weight_method=cfg['sample_weight_method'])\n        current_val_dataset = SequencepHDataset(val_df, sample_weight_method='None')\n        # Create DataLoaders with suggested batch size\n        train_loader = DataLoader(current_train_dataset, batch_size=cfg['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n        val_loader = DataLoader(current_val_dataset, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n        print(f\"Trial {trial.number}: Train batches={len(train_loader)}, Val batches={len(val_loader)}\")\n    except Exception as e:\n         print(f\"Trial {trial.number} failed during data loading: {e}\")\n         return float('inf') # Indicate failure\n\n    # Build model using suggested hyperparameters\n    try:\n        model = SequenceTopHModel(\n            vocab_size=VOCAB_SIZE, lstm_embedding_dim=cfg['lstm_emb_dim'], lstm_hidden_dim=cfg['lstm_hidden_dim'],\n            num_lstm_layers=cfg['lstm_layers'], lstm_dropout=cfg['lstm_dropout'], rlat_kernel_size=cfg['rlat_kernel_size'],\n            rlat_dropout=cfg['rlat_dropout'], rlat_res_blocks=cfg['rlat_res_blocks'], rlat_activation=cfg['rlat_activation'],\n            random_seed=trial_seed\n        ).to(device)\n        print(f\"Trial {trial.number}: Model params={model.get_num_params():,}\")\n    except Exception as e:\n        print(f\"Trial {trial.number} failed during model creation: {e}\")\n        # Clean up data loaders if model fails\n        del train_loader, val_loader, current_train_dataset, current_val_dataset\n        gc.collect(); torch.cuda.empty_cache()\n        return float('inf') # Indicate failure\n\n\n    # Optimizer using suggested name and LR/Reg\n    if cfg['optimizer_name'] == 'Adam':\n        optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'], weight_decay=cfg['l2_reg'])\n    else: # AdamW\n         optimizer = optim.AdamW(model.parameters(), lr=cfg['learning_rate'], weight_decay=cfg['l2_reg'])\n\n    loss_fn = weighted_rmse_loss\n    # Use fixed scheduler params or tune them as well (keeping fixed for now)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=REDUCE_LR_PATIENCE, verbose=False, min_lr=1e-7)\n\n    # --- 3. Training Loop for this Trial (with reduced HPO epochs) ---\n    best_trial_val_loss = float('inf') # Track best loss *for this trial*\n    epochs_no_improve = 0\n    trial_last_epoch = 0\n\n    # Reduced number of epochs per HPO trial\n    HPO_EPOCHS = 60 # Adjust this value (e.g., 50-75) based on time constraints\n\n    print(f\"Trial {trial.number}: Starting training for max {HPO_EPOCHS} epochs...\")\n    for epoch in range(1, HPO_EPOCHS + 1):\n        trial_last_epoch = epoch\n        epoch_start_time = time.time()\n\n        # Train one epoch\n        model.train()\n        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device) # Assumes defined above\n\n        # Validate one epoch\n        model.eval()\n        current_val_loss, val_metrics = validate_epoch(model, val_loader, loss_fn, device) # Assumes defined above\n\n        epoch_duration = time.time() - epoch_start_time\n        # Print progress minimally during HPO\n        if epoch % 10 == 0 or epoch == 1: # Print every 10 epochs and the first epoch\n             print(f\"  Epoch {epoch}/{HPO_EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {current_val_loss:.4f} | Val RMSE: {val_metrics.get('rmse', float('nan')):.4f} | Time: {epoch_duration:.1f}s\")\n\n        # Handle potential NaN validation loss\n        if np.isnan(current_val_loss) or np.isinf(current_val_loss):\n             print(f\"Trial {trial.number} - Epoch {epoch}: Invalid validation loss ({current_val_loss}). Pruning.\")\n             del model, optimizer, train_loader, val_loader, current_train_dataset, current_val_dataset\n             gc.collect(); torch.cuda.empty_cache()\n             raise optuna.TrialPruned()\n\n        # Update best loss *for this trial*\n        if current_val_loss < best_trial_val_loss:\n            best_trial_val_loss = current_val_loss\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n\n        # Step the scheduler\n        scheduler.step(current_val_loss)\n\n        # --- Optuna Pruning Check ---\n        trial.report(current_val_loss, epoch)\n        if trial.should_prune():\n            print(f\"Trial {trial.number} pruned at epoch {epoch} with val_loss {current_val_loss:.4f}.\")\n            del model, optimizer, train_loader, val_loader, current_train_dataset, current_val_dataset\n            gc.collect(); torch.cuda.empty_cache()\n            raise optuna.TrialPruned()\n\n        # --- Early Stopping for this Trial ---\n        if epochs_no_improve >= STOP_PATIENCE: # Use global STOP_PATIENCE\n             print(f\"Trial {trial.number} early stopped at epoch {epoch} due to lack of improvement (best loss: {best_trial_val_loss:.4f}).\")\n             break # Stop training this specific trial\n\n    # --- 4. Return Metric to Optimize ---\n    print(f\"Trial {trial.number} completed after {trial_last_epoch} epochs. Best val_loss: {best_trial_val_loss:.4f}\")\n    del model, optimizer, train_loader, val_loader, current_train_dataset, current_val_dataset\n    gc.collect(); torch.cuda.empty_cache()\n\n    # Return the best validation LOSS achieved during this trial\n    return best_trial_val_loss\n\n\n# --- Run the Optuna Study ---\nN_TRIALS = 30 # Reduced number of trials initially to test stability, increase later (e.g., 50)\nSTUDY_NAME = f\"{MODEL_NAME}_HPO_v2\" # New study name\n\n# --- Start a NEW study ---\n# If you interrupted a previous one, it's often cleaner to start fresh with adjusted ranges\nstudy = optuna.create_study(\n    direction='minimize', # Minimize validation loss\n    study_name=STUDY_NAME,\n    sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n    # Use a more aggressive pruner\n    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=15, interval_steps=3) # Check pruning sooner\n)\n\nprint(f\"\\nStarting Optuna optimization with {N_TRIALS} trials...\")\ntry:\n    study.optimize(\n        objective,\n        n_trials=N_TRIALS,\n        timeout=None, # e.g., 6 * 3600 for 6 hours\n        gc_after_trial=True # Helps manage memory\n    )\nexcept KeyboardInterrupt:\n     print(\"Optimization stopped manually.\")\nexcept Exception as e:\n     print(f\"An error occurred during Optuna optimization: {e}\")\n     import traceback\n     traceback.print_exc()\n\n\n# --- Optimization Finished ---\nprint(\"\\n--- Optuna Optimization Finished ---\")\nprint(f\"Number of finished trials: {len(study.trials)}\")\n\ntry:\n    best_trial = study.best_trial\n    print(f\"Best trial number: {best_trial.number}\")\n    print(f\"Best validation objective value (loss): {best_trial.value:.4f}\")\n    print(\"Best hyperparameters:\")\n    for key, value in best_trial.params.items():\n        print(f\"  {key}: {value}\")\n    BEST_PARAMS = best_trial.params\n    print(\"\\nBest parameters stored in BEST_PARAMS dictionary.\")\n\nexcept ValueError: # Handles case where no trials completed successfully\n     print(\"No successful trials completed. Cannot determine best parameters.\")\n     BEST_PARAMS = None\n\n# Save study results\nstudy_results_df = study.trials_dataframe()\nstudy_results_path = os.path.join(SAVEDIR, f\"{STUDY_NAME}_results.csv\")\nstudy_results_df.to_csv(study_results_path, index=False)\nprint(f\"\\nStudy results saved to {study_results_path}\")\n\n\n# --- Proceed to Cell 10 (Retraining with BEST_PARAMS if available) ---\nif BEST_PARAMS is None:\n    print(\"\\nSkipping final retraining as no best parameters were found.\")\nelse:\n    print(\"\\nProceeding to final retraining using BEST_PARAMS (in Cell 10).\")\n# (Ensure Cell 10 exists and uses the BEST_PARAMS dictionary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:50.577500Z","iopub.execute_input":"2025-04-24T02:02:50.577711Z","iopub.status.idle":"2025-04-24T02:02:53.547576Z","shell.execute_reply.started":"2025-04-24T02:02:50.577696Z","shell.execute_reply":"2025-04-24T02:02:53.546457Z"}},"outputs":[{"name":"stdout","text":"--- Setting up Optuna Hyperparameter Optimization (Tuned Ranges) ---\nUsing device: cuda\nERROR: train_df or val_df not defined. Please execute data loading cell first.\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"],"ename":"SystemExit","evalue":"1","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"print(study.trials_dataframe())\n# Look specifically at the row for trial number 2 if it exists\n# Or look at the last successful trial's info\nprint(\"\\nBest Trial So Far:\")\nprint(study.best_trial)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:53.548211Z","iopub.status.idle":"2025-04-24T02:02:53.548507Z","shell.execute_reply.started":"2025-04-24T02:02:53.548371Z","shell.execute_reply":"2025-04-24T02:02:53.548382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code]\n# =============================================================================\n# 10. Retrain Model with Best Hyperparameters & Evaluate on Test Set\n# =============================================================================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport time\nimport os\nimport sys\nimport gc # Import garbage collector\n\n# --- Ensure Previous Cells containing necessary definitions are executed ---\n# Requires: BEST_PARAMS dict, train_df, val_df, test_df pandas DataFrames,\n#           SequenceTopHModel, SequencepHDataset, collate_fn, weighted_rmse_loss,\n#           train_one_epoch, validate_epoch, VOCAB_SIZE, PAD_ID, SAVEDIR,\n#           MODEL_NAME, RANDOM_SEED, NUM_WORKERS, EPOCHS, REDUCE_LR_PATIENCE,\n#           STOP_PATIENCE\n\nprint(\"\\n--- Retraining model with best hyperparameters ---\")\n\n# --- Check if BEST_PARAMS exists ---\nif 'BEST_PARAMS' not in globals() or BEST_PARAMS is None:\n    print(\"ERROR: BEST_PARAMS dictionary not found or is None. Cannot proceed.\")\n    print(\"Please run the Optuna study successfully in Cell 9 first.\")\n    # Stop execution if params aren't available\n    # sys.exit(1) # Uncomment for non-interactive runs\n    # In a notebook, better to raise an error or just print the message and stop here.\n    raise NameError(\"BEST_PARAMS not defined. Run Optuna study cell.\")\nelse:\n    print(\"Using best parameters found by Optuna:\")\n    # Print cleanly\n    for key, value in BEST_PARAMS.items():\n        print(f\"  {key}: {value}\")\n\n    # --- Setup using BEST_PARAMS ---\n    final_cfg = BEST_PARAMS.copy() # Use the best params found\n    final_seed = RANDOM_SEED # Use the global seed for consistency\n    final_model_name = f\"{MODEL_NAME}_best_hpo\" # Distinct name for the final model\n\n    # Set seed for final training run\n    torch.manual_seed(final_seed)\n    np.random.seed(final_seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if device == torch.device(\"cuda\"):\n        torch.cuda.manual_seed_all(final_seed)\n\n    # --- Create Datasets and Dataloaders for final run ---\n    try:\n        # Ensure dataframes exist\n        if 'train_df' not in globals() or 'val_df' not in globals() or 'test_df' not in globals():\n             raise NameError(\"train_df, val_df, or test_df not found. Ensure data loading in Cell 9 ran correctly.\")\n\n        print(\"Creating final datasets...\")\n        final_train_dataset = SequencepHDataset(train_df, sample_weight_method=final_cfg['sample_weight_method'])\n        final_val_dataset = SequencepHDataset(val_df, sample_weight_method='None')\n        final_test_dataset = SequencepHDataset(test_df, sample_weight_method='None')\n\n        # Use the batch size found by Optuna\n        final_batch_size = final_cfg['batch_size']\n        print(f\"Using final batch size: {final_batch_size}\")\n\n        final_train_loader = DataLoader(final_train_dataset, batch_size=final_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)\n        final_val_loader = DataLoader(final_val_dataset, batch_size=final_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=True)\n        # Use a potentially larger batch size for test evaluation if memory allows\n        final_test_batch_size = max(64, final_batch_size) # Example: Use 64 or the training batch size, whichever is larger\n        final_test_loader = DataLoader(final_test_dataset, batch_size=final_test_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n        print(\"Final DataLoaders created.\")\n\n    except Exception as e:\n        print(f\"Error creating final datasets/loaders: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Stop if datasets fail\n        # sys.exit(1) # Uncomment for non-interactive runs\n        raise RuntimeError(\"Failed to create final datasets/loaders.\") from e\n\n    # --- Build final model using BEST_PARAMS ---\n    try:\n        print(\"Building final model...\")\n        final_model = SequenceTopHModel(\n            vocab_size=VOCAB_SIZE,\n            lstm_embedding_dim=final_cfg['lstm_emb_dim'],\n            lstm_hidden_dim=final_cfg['lstm_hidden_dim'],\n            num_lstm_layers=final_cfg['lstm_layers'],\n            lstm_dropout=final_cfg['lstm_dropout'],\n            rlat_kernel_size=final_cfg['rlat_kernel_size'],\n            rlat_dropout=final_cfg['rlat_dropout'],\n            rlat_res_blocks=final_cfg['rlat_res_blocks'],\n            rlat_activation=final_cfg['rlat_activation'],\n            random_seed=final_seed\n        ).to(device)\n        print(f\"Final model created with {final_model.get_num_params():,} parameters.\")\n    except Exception as e:\n        print(f\"Error building final model: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise RuntimeError(\"Failed to build final model.\") from e\n\n\n    # --- Final Optimizer and Loss ---\n    try:\n        print(\"Setting up final optimizer and scheduler...\")\n        if final_cfg['optimizer_name'] == 'Adam':\n            final_optimizer = optim.Adam(final_model.parameters(), lr=final_cfg['learning_rate'], weight_decay=final_cfg['l2_reg'])\n        elif final_cfg['optimizer_name'] == 'AdamW':\n            final_optimizer = optim.AdamW(final_model.parameters(), lr=final_cfg['learning_rate'], weight_decay=final_cfg['l2_reg'])\n        else: # Default fallback\n            print(f\"Warning: Unknown optimizer '{final_cfg['optimizer_name']}'. Defaulting to Adam.\")\n            final_optimizer = optim.Adam(final_model.parameters(), lr=final_cfg['learning_rate'], weight_decay=final_cfg['l2_reg'])\n\n        final_loss_fn = weighted_rmse_loss\n        # Scheduler monitors validation RMSE\n        final_scheduler = ReduceLROnPlateau(final_optimizer, mode='min', factor=0.5, patience=REDUCE_LR_PATIENCE, verbose=True, min_lr=1e-7)\n        print(\"Optimizer and scheduler ready.\")\n    except Exception as e:\n        print(f\"Error setting up optimizer/scheduler: {e}\")\n        raise RuntimeError(\"Failed to setup optimizer/scheduler.\") from e\n\n    # --- Final Training Loop ---\n    print(\"\\n--- Starting Final Training Loop ---\")\n    best_final_val_rmse = float('inf') # Track best RMSE for saving\n    final_epochs_no_improve = 0\n    final_training_log = []\n    final_model_save_path = os.path.join(SAVEDIR, f\"{final_model_name}.pt\") # Define save path\n\n    start_time_total = time.time()\n    last_epoch = 0 # Keep track of the last epoch number\n\n    try:\n        for epoch in range(1, EPOCHS + 1):\n            last_epoch = epoch\n            start_time_epoch = time.time()\n            print(f\"\\n--- Final Training Epoch {epoch}/{EPOCHS} ---\")\n\n            # Training Step\n            final_model.train()\n            train_loss = train_one_epoch(final_model, final_train_loader, final_optimizer, final_loss_fn, device)\n\n            # Validation Step\n            final_model.eval()\n            val_loss, val_metrics = validate_epoch(final_model, final_val_loader, final_loss_fn, device)\n            # Use validation RMSE as the primary metric to check for improvement\n            current_val_rmse = val_metrics.get('rmse', float('inf'))\n\n            # Handle potential NaN in validation metric before using it\n            if np.isnan(current_val_rmse) or np.isinf(current_val_rmse):\n                 print(f\"Warning: Validation RMSE is {current_val_rmse}. Skipping scheduler step and model saving.\")\n                 # Decide how to handle this: continue, count as non-improvement, or break\n                 final_epochs_no_improve += 1 # Example: Count as non-improvement\n            else:\n                # Step the scheduler based on the valid validation RMSE\n                final_scheduler.step(current_val_rmse)\n\n                # --- Save model and handle early stopping BASED ON VALIDATION RMSE ---\n                if current_val_rmse < best_final_val_rmse:\n                    print(f\"  Validation RMSE improved ({best_final_val_rmse:.4f} --> {current_val_rmse:.4f}). Saving model to {final_model_save_path}\")\n                    best_final_val_rmse = current_val_rmse # Update best RMSE\n                    final_epochs_no_improve = 0\n                    # Save the best model's state dictionary\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': final_model.state_dict(),\n                        'optimizer_state_dict': final_optimizer.state_dict(),\n                        'best_val_rmse': best_final_val_rmse,\n                        'final_val_metrics': val_metrics, # Save metrics dict at best point\n                        'hyperparameters': BEST_PARAMS # Save the HPO params used\n                    }, final_model_save_path)\n                else:\n                    final_epochs_no_improve += 1\n                    print(f\"  Validation RMSE did not improve for {final_epochs_no_improve} epoch(s). Best: {best_final_val_rmse:.4f}\")\n\n            # Print Epoch Summary\n            epoch_duration = time.time() - start_time_epoch\n            print(f\"Epoch {epoch} Summary:\")\n            print(f\"  Train Loss: {train_loss:.4f}\")\n            print(f\"  Val Loss:   {val_loss:.4f}\") # Report raw validation loss\n            print(f\"  Val RMSE:   {current_val_rmse:.4f}\") # Report validation RMSE used for decisions\n            print(f\"  Val R2:     {val_metrics.get('r2', float('nan')):.4f}\")\n            print(f\"  Learning Rate: {final_optimizer.param_groups[0]['lr']:.2e}\")\n            print(f\"  Duration:   {epoch_duration:.2f}s\")\n\n            # Log results for this epoch\n            log_entry = {\n                'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss,\n                'val_rmse': current_val_rmse, 'val_r2': val_metrics.get('r2', float('nan')),\n                'val_rho': val_metrics.get('rho', float('nan')), # Log other metrics too\n                'learning_rate': final_optimizer.param_groups[0]['lr'], 'duration': epoch_duration\n            }\n            final_training_log.append(log_entry)\n\n            # Check for early stopping\n            if final_epochs_no_improve >= STOP_PATIENCE:\n                print(f\"\\nFinal training early stopped after {STOP_PATIENCE} epochs without improvement on validation RMSE.\")\n                break\n            # Optional: Check for NaN stop\n            # if np.isnan(current_val_rmse) or np.isinf(current_val_rmse):\n            #     print(f\"Stopping final training due to invalid validation RMSE: {current_val_rmse}\")\n            #     break\n\n    except Exception as e:\n        print(f\"\\nAn error occurred during the final training loop at epoch {last_epoch}: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        # --- End of Final Training ---\n        total_duration = time.time() - start_time_total\n        print(f\"\\n--- Final Training Finished ---\")\n        print(f\"Training ran for {last_epoch} epochs.\")\n        print(f\"Total final training time: {total_duration/60:.2f} minutes.\")\n        if np.isinf(best_final_val_rmse):\n             print(\"Best validation RMSE could not be determined (remained infinity).\")\n        else:\n             print(f\"Best final validation RMSE achieved: {best_final_val_rmse:.4f}\")\n\n        # Save final log regardless of how loop ended\n        if final_training_log:\n             log_df = pd.DataFrame(final_training_log)\n             log_path = os.path.join(SAVEDIR, f\"{final_model_name}_training_log.csv\")\n             log_df.to_csv(log_path, index=False)\n             print(f\"Final training log saved to {log_path}\")\n        else:\n             print(\"No epochs completed, final training log not saved.\")\n\n        # Clean up GPU memory\n        del final_model, final_optimizer, final_train_loader, final_val_loader, final_train_dataset, final_val_dataset\n        gc.collect()\n        if device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n\n\n    # --- Final Evaluation on Test Set ---\n    print(\"\\n--- Evaluating final model on Test Set ---\")\n    # Use the path where the best model *should* have been saved\n    if os.path.exists(final_model_save_path) and not np.isinf(best_final_val_rmse):\n        print(f\"Loading best model from: {final_model_save_path}\")\n        try:\n            # Load the checkpoint\n            checkpoint = torch.load(final_model_save_path, map_location=device)\n            # Verify hyperparameters match BEST_PARAMS\n            if checkpoint['hyperparameters'] != BEST_PARAMS:\n                 print(\"Warning: Hyperparameters in saved model do not match BEST_PARAMS from Optuna study!\")\n\n            # Rebuild model architecture with saved hyperparameters\n            # Use the hyperparameters stored *in the checkpoint* for robustness\n            saved_hyperparams = checkpoint['hyperparameters']\n            eval_model = SequenceTopHModel(\n                 vocab_size=VOCAB_SIZE, lstm_embedding_dim=saved_hyperparams['lstm_emb_dim'],\n                 lstm_hidden_dim=saved_hyperparams['lstm_hidden_dim'], num_lstm_layers=saved_hyperparams['lstm_layers'],\n                 lstm_dropout=saved_hyperparams['lstm_dropout'], rlat_kernel_size=saved_hyperparams['rlat_kernel_size'],\n                 rlat_dropout=saved_hyperparams['rlat_dropout'], rlat_res_blocks=saved_hyperparams['rlat_res_blocks'],\n                 rlat_activation=saved_hyperparams['rlat_activation'], random_seed=final_seed # Use consistent seed\n             ).to(device)\n\n            # Load the learned weights\n            eval_model.load_state_dict(checkpoint['model_state_dict'])\n            eval_model.eval() # Set to evaluation mode\n\n            print(\"Model loaded successfully. Running test evaluation...\")\n            # Run validation function on the test loader\n            test_loss, test_metrics = validate_epoch(eval_model, final_test_loader, final_loss_fn, device)\n\n            print(\"\\n--- Final Model Test Set Performance ---\")\n            print(f\"  Test Loss (Weighted RMSE): {test_loss:.4f}\") # Note: This is weighted RMSE loss on test set\n            print(f\"  Test RMSE (Eval Weighted): {test_metrics.get('rmse', float('nan')):.4f}\") # From performance()\n            print(f\"  Test R2   (Eval Weighted): {test_metrics.get('r2', float('nan')):.4f}\")\n            print(f\"  Test Rho  (Spearman):      {test_metrics.get('rho', float('nan')):.4f}\")\n            print(f\"  Test R    (Pearson):       {test_metrics.get('r', float('nan')):.4f}\")\n            print(f\"  Test MCC  (Binned):        {test_metrics.get('mcc', float('nan')):.4f}\")\n            print(f\"  Test F1   (Binned, W-Avg): {test_metrics.get('f1score', float('nan')):.4f}\")\n            print(f\"  Test AUC  (Binned, OvR):   {test_metrics.get('auc', float('nan')):.4f}\")\n\n            # Clean up eval model\n            del eval_model, checkpoint\n            gc.collect()\n            if device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n\n        except Exception as e:\n            print(f\"Error during final test evaluation: {e}\")\n            import traceback\n            traceback.print_exc()\n    elif np.isinf(best_final_val_rmse):\n         print(f\"Skipping test evaluation because final model training did not achieve a valid best validation RMSE.\")\n    else:\n        print(f\"Final best model file not found at {final_model_save_path}. Cannot perform test evaluation.\")\n\n# Final message if BEST_PARAMS was initially None\nif 'BEST_PARAMS' not in globals() or BEST_PARAMS is None:\n     print(\"\\nRetraining and evaluation skipped as BEST_PARAMS were not available.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T02:02:53.549829Z","iopub.status.idle":"2025-04-24T02:02:53.550157Z","shell.execute_reply.started":"2025-04-24T02:02:53.549993Z","shell.execute_reply":"2025-04-24T02:02:53.550008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ## Next Steps: Inference\n#\n# After training, you can use the saved best model (`_best.pt`) to predict pH for new sequences. You would typically:\n# 1.  Load the model architecture (SequenceTopHModel).\n# 2.  Load the saved state dictionary (`torch.load(...)`).\n# 3.  Set the model to evaluation mode (`model.eval()`).\n# 4.  Adapt the logic from `run.py` (reading FASTA, tokenizing, padding, running model inference) in a new cell or notebook.","metadata":{}}]}